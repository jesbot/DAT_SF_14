@misc{deep,
  title = {{Theano Convolution Network}},
  howpublished = {\url{http://deeplearning.net/tutorial/lenet.html}}
}
@misc{kaggle,
  title = {Kaggle},
  howpublished = {\url{https://www.kaggle.com/}}
}
@misc{nih,
  title = {{NIH National Eye Institute}},
  howpublished = {\url{https://nei.nih.gov/health/diabetic/retinopathy}}
}
@misc{wiki,
  title = {{Diabetic Retinopathy Wiki Entry}},
  howpublished = {\url{http://en.wikipedia.org/wiki/Diabetic_retinopathy}}
}
@misc{lena,
  title = {{IEEE Computer Society}},
  howpublished = {\url{http://www.computer.org/csdl/trans/tc/2013/04/ttc2013040631-abs.html}}
}
@article{LeNet,
author = {LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.},
journal = {Proceedings of the IEEE},
number = {86},
pages = {2278–2324},
title = {{Gradient-based learning applied to document recognition}},
volume = {11},
year = {1998}
}
@misc{cs231,
  title = {{Stanford Computer Science Dept: Class 231 -Convolution Neural Networks}},
  howpublished = {\url{https://cs231n.github.io/}}
}
@misc{ann,
  title = {{wikiBooks: Artificial Neural Networks}},
  howpublished = {\url{http://en.wikibooks.org/wiki/Artificial_Neural_Networks/Print_Version}}
}
@article{caffe,
  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal = {arXiv preprint arXiv:1408.5093},
  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  Year = {2014}
}
@incollection{AlexNet,
title = {ImageNet Classification with Deep Convolutional Neural Networks},
author = {Alex Krizhevsky and Sutskever, Ilya and Geoffrey E. Hinton},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {F. Pereira and C.J.C. Burges and L. Bottou and K.Q. Weinberger},
pages = {1097--1105},
year = {2012},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}
@article{caffe,
  Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  Journal = {arXiv preprint arXiv:1408.5093},
  Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},
  Year = {2014}
}
@article{GoogLeNet,
  author    = {Christian Szegedy and
               Wei Liu and
               Yangqing Jia and
               Pierre Sermanet and
               Scott Reed and
               Dragomir Anguelov and
               Dumitru Erhan and
               Vincent Vanhoucke and
               Andrew Rabinovich},
  title     = {Going Deeper with Convolutions},
  journal   = {CoRR},
  volume    = {abs/1409.4842},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.4842},
  timestamp = {Wed, 01 Oct 2014 15:00:05 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SzegedyLJSRAEVR14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@misc{digits,
  title = {{Digits Deep Learning Architecture}},
  howpublished = {\url{https://developer.nvidia.com/digits}}
}
@INPROCEEDINGS{theano,
     author = {Bergstra, James and Breuleux, Olivier and Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu, Razvan and Desjardins, Guillaume and Turian, Joseph and Warde-Farley, David and Bengio, Yoshua},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the Python for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
       note = {Oral Presentation},
   abstract = {Theano is a compiler for mathematical expressions in Python that combines the convenience of NumPy’s syntax with the speed of optimized native machine language. The user composes mathematical expressions in a high-level description that mimics NumPy’s syntax and semantics, while being statically typed and
functional (as opposed to imperative). These expressions allow Theano to provide symbolic differentiation. Before performing computation, Theano optimizes the choice of expressions, translates
them into C++ (or CUDA for GPU), compiles them into dynamically loaded Python modules, all automatically. Common machine learning algorithms implemented with Theano are from 1.6× to 7.5× faster than competitive alternatives (including those implemented with C/C++, NumPy/SciPy and MATLAB) when compiled for the
CPU and between 6.5× and 44× faster when compiled for the GPU. This paper illustrates how to use Theano, outlines the scope of the compiler, provides benchmarks on both CPU and GPU processors, and explains its overall design.}
}